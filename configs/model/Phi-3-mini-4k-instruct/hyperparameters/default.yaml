trainer_kwargs: # passed to TrainingArguments
  # Memory optimization
  bf16: True

  # Batch size
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 1

  # Core hyperparameters
  learning_rate: 5.e-4
  num_train_epochs: 25

  # Evaluation
  eval_strategy: steps
  eval_steps: 0.2

  # Logging
  logging_strategy: steps
  logging_steps: 0.2


  # Early stopping
  save_strategy: epoch
  save_total_limit: 1

  # Outputs
  output_dir: output

peft_kwargs:  # passed to LoraConfig
  r: 8
  target_modules: ["k_proj", "q_proj", "v_proj", "o_proj", "gate_proj", "down_proj", "up_proj"]
